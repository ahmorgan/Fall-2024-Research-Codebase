# OUR Research Scholars Fall 24
# Comparative Analysis of Resource-Efficient Multi-Label Text Classification Methods: GPT-4o vs fine-tuning with SetFit
This repository contains all code created for the above project. I could not include the train/test datasets I used for SetFit/GPT-4o or the raw data I used for train/test dataset construction for research ethics purposes.

Abstract:

Text classification – the automatic pairing of text with labels – has proven itself a ubiquitous natural language processing task. The use of large language models (LLMs) fine-tuned on text data (in particular, BERT and its derivatives) is a common method for performing this task, as is prompting LLMs, such as OpenAI’s GPT models. One particular advancement in the problem of text classification has been the free SetFit (Sentence Fine-Tuning) framework pioneered by Hugging Face, shown to offset both training data requirements, only needing as little as eight examples per classification category to accurately fine-tune, and the computational cost that often comes with fine-tuning, while still providing competitive accuracy. Notably, their results show that a fine-tuned ~100M parameter SetFit model surpasses GPT-3 by about 4.2%. However, OpenAI has since released GPT-4o, which greatly improves upon GPT-3 and costs only about 14 cents per 100 classifications using our prompt. We seek to compare these two cost- and compute-efficient methods – GPT-4o and SetFit – of multi-label classification and examine SetFit two years after its release. We novelly compare the performance of a SetFit model to GPT-4o on a multi-label classification task involving classifying student reflections from a software engineering course at UNCC with one or more common course struggles – furthermore, we do so with the constraint that both methods of classification must be performable on average consumer-grade hardware (we use an 8GB NVIDIA GPU). Our results show that GPT-4o outperforms SetFit in overall accuracy. We also examine the confusion matrices generated by each method.
